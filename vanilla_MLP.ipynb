{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%pdb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LogisticSigmoid(X):\n",
    "    \"\"\"\n",
    "    Computes the sigmoid activation function.\"\"\"\n",
    "\n",
    "    return 1.0/(1+ np.exp(-X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Softmax(Y):\n",
    "    \"\"\"\n",
    "    Computes the softmax activation function for classification.\"\"\"\n",
    "\n",
    "    return np.exp(Y) / np.sum(np.exp(Y), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def CrossEntropyLoss(Y, Y_true):\n",
    "    \"\"\"\n",
    "    Computes the cross entropy loss function.\"\"\"\n",
    "\n",
    "    cost_sum = np.sum(np.multiply(Y_true, np.log(Y)))\n",
    "    m = Y.shape[0]\n",
    "    cost = -(1/m) * cost_sum\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_empty(alist):\n",
    "    if alist:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MultiLayerPeceptron:\n",
    "    \"\"\"creating Multi-layer perceptron (feedforward Neural network) model class.\"\"\"\n",
    "    def __init__(self, x_input, y_input, nb_class):\n",
    "        self.input = x_input                  # Input data (matrix)\n",
    "        self.y = y_input                      # Output data (vector)\n",
    "        self.nb_class = nb_class              # Number of output class\n",
    "        self.weights_vectors = []             # Creating all the wij weights matrices\n",
    "        self.hidden_vectors = []              # Creating model's hidden vectors\n",
    "        self.bias_vectors = []                # Creating model's bias vectors (associated to hidden vectors)\n",
    "\n",
    "        self.momentum_weights_vectors = []\n",
    "        self.momentum_bias_vectors = []\n",
    "\n",
    "        self.output = np.zeros(self.nb_class) # Initial output vector (prediction) \n",
    "        self.output_indicator = True          # To initialize output weights and bias\n",
    "        self.momentum_indicator = True        # To initialize momentum term\n",
    "        self.dropout_indicator = []           # Boolean vector to indicate dropout\n",
    "        \n",
    "    def addHiddenLayer(self, nb_neurons, dropout=0.):\n",
    "        \"\"\"Adding hidden layers to model by creating new weights matrices. Weights are\n",
    "        randomly initialized.\"\"\"\n",
    "\n",
    "        # First hidden layer case\n",
    "        if is_empty(self.weights_vectors):\n",
    "            weights = np.random.rand(nb_neurons, self.input.shape[1])* np.sqrt(2. / self.input.shape[1] + nb_neurons) # Adjusting weights variance\n",
    "\n",
    "        # All other hidden layer case\n",
    "        else:\n",
    "            weights = np.random.rand(nb_neurons, self.weights_vectors[-1].shape[0])*np.sqrt(2. / self.weights_vectors[-1].shape[0] + nb_neurons)\n",
    "\n",
    "        hidden_vector = np.random.rand(nb_neurons) # Creates hidden layer (random activations)\n",
    "        bias_vector = np.ones((nb_neurons,1))      # Creates bias vectors\n",
    "\n",
    "        self.hidden_vectors.append(hidden_vector)\n",
    "        self.bias_vectors.append(bias_vector)\n",
    "        self.weights_vectors.append(weights)\n",
    "        \n",
    "        # Initialises dropout\n",
    "        if dropout == 0. :\n",
    "            # Default case (no dropout)\n",
    "            self.dropout_indicator.append((False, 0.))                                   \n",
    "        else:\n",
    "            self.dropout_indicator.append((True, dropout)) # Dropout case \n",
    "            \n",
    "\n",
    "        return self\n",
    "\n",
    "    def feedforward(self, x_input):\n",
    "        \"\"\"Compute feedforward propagation from input data tensor x_input. Logistic sigmoid for hidden layers \n",
    "         and softmax activation fuction for output layer are considered.\"\"\"\n",
    "        \n",
    "        # Creates initial random weight for the output layers\n",
    "        # Executes only once\n",
    "        \n",
    "        if self.output_indicator:\n",
    "\n",
    "            weights = np.random.rand(self.output.shape[0], self.weights_vectors[-1].shape[0])* np.sqrt(2. / self.weights_vectors[-1].shape[0] + self.output.shape[0])\n",
    "            bias_vector = np.ones((self.nb_class,1))\n",
    "\n",
    "            self.bias_vectors.append(bias_vector)\n",
    "            self.weights_vectors.append(weights)\n",
    "\n",
    "            self.output_indicator = False\n",
    "        \n",
    "        x = x_input\n",
    "        for nb_layers in range(len(self.hidden_vectors)):\n",
    "\n",
    "            b = np.asarray(self.bias_vectors[nb_layers])          # Current bias\n",
    "            W = np.asarray(self.weights_vectors[nb_layers])       # Current weights\n",
    "      \n",
    "            h_fwd = LogisticSigmoid(np.matmul(W,x.T) + b)         # Computes forward hidden layer activation\n",
    "                \n",
    "            #Add dropout\n",
    "            if self.dropout_indicator[nb_layers][0]:                # Check if dropout is not null for current hidden layer\n",
    "                dropout_rate = self.dropout_indicator[nb_layers][1] # Get dropout rate from tuple\n",
    "                keep_prob = 1-dropout_rate\n",
    "                d = np.random.rand(self.hidden_vectors[nb_layers].shape[0],1) # Create boolean with randomnly kept units\n",
    "                d = d < keep_prob                                            \n",
    "                h_fwd = np.prod((h_fwd, d), axis=0)        # Prune activation neurons w.r.t dropout rate\n",
    "                h_fwd = h_fwd/keep_prob                    # Scale the value of neurons that haven't been shut down\n",
    "                              \n",
    "            h_fwd = h_fwd.T                                # re-setting dimesions \n",
    "            \n",
    "            #Updating activations\n",
    "            self.hidden_vectors[nb_layers] = h_fwd        # Update object hidden layer attributes\n",
    "            x = h_fwd\n",
    "                  \n",
    "        # Updating output\n",
    "        W_out = np.asarray(self.weights_vectors[-1])\n",
    "        b_out = np.asarray(self.bias_vectors[-1])\n",
    "        \n",
    "        self.output = Softmax(np.matmul(W_out,x.T) + b_out)\n",
    "    \n",
    "        y_output = self.output\n",
    "\n",
    "        return self, y_output\n",
    "    \n",
    "    def backpropagation(self, y_output, y_true, lr, beta, x_input):\n",
    "        \"\"\"Computes backpropagation from predicted output and true label. \n",
    "        - lr   : represents the gradiant algorithm learning rate\n",
    "        - beta : represents the momentum term coefficient.\"\"\"\n",
    "\n",
    "        m = y_output.shape[0] # get batch size\n",
    "\n",
    "        y_error = np.asarray(y_output - y_true)\n",
    "        \n",
    "        ## Computes backpropagation for output layer (computed for Softmax)\n",
    "        dW_out = (1./m)*np.matmul((y_error).T, self.hidden_vectors[-1])\n",
    "        db_out = (1./m)*np.sum(y_error, axis=0, keepdims=True).T\n",
    "        \n",
    "        # initializes momentum terms (run once)\n",
    "        if self.momentum_indicator:\n",
    "            \n",
    "            # Creating momentum terms vectors for hidden layers\n",
    "            for nb_layers in range(len(self.hidden_vectors)):\n",
    "                V_dW = np.zeros(self.weights_vectors[nb_layers].shape)\n",
    "                V_db = np.zeros(self.bias_vectors[nb_layers].shape)\n",
    "\n",
    "                self.momentum_weights_vectors.append(V_dW)\n",
    "                self.momentum_bias_vectors.append(V_db)\n",
    "            \n",
    "            # Creating momentum terms vectors for output layer\n",
    "            V_dW_out = np.zeros(self.weights_vectors[-1].shape)\n",
    "            V_db_out = np.zeros(self.bias_vectors[-1].shape)\n",
    "\n",
    "            self.momentum_weights_vectors.append(V_dW_out)\n",
    "            self.momentum_bias_vectors.append(V_db_out)\n",
    "\n",
    "            self.momentum_indicator = False\n",
    "        \n",
    "        # compute momentum terms for output\n",
    "        V_dW_out = np.asarray(self.momentum_weights_vectors[-1])\n",
    "        V_db_out = np.asarray(self.momentum_bias_vectors[-1])\n",
    "        \n",
    "        self.momentum_weights_vectors[-1] = (beta * V_dW_out + (1. - beta) * dW_out)\n",
    "        self.momentum_bias_vectors[-1] = (beta * V_db_out  + (1. - beta) * db_out)\n",
    "        \n",
    "        # Update output parameter using momentum\n",
    "        W_out = np.asarray(self.weights_vectors[-1])\n",
    "        b_out = np.asarray(self.bias_vectors[-1])\n",
    "                \n",
    "        V_dW_out2 = np.asarray(self.momentum_weights_vectors[-1])\n",
    "        V_db_out2 = np.asarray(self.momentum_bias_vectors[-1])\n",
    "\n",
    "        self.weights_vectors[-1] = W_out - lr * V_dW_out2\n",
    "        self.bias_vectors[-1]    = b_out - lr * V_db_out2\n",
    "        \n",
    "        y_error = y_error.T\n",
    "        dh_L = y_error\n",
    "        \n",
    "        ## Computes backpropagation for all other layers (computed for Sigmoid)\n",
    "        for nb_layers in reversed(range(len(self.hidden_vectors))):\n",
    "            \n",
    "            h = np.asarray(self.hidden_vectors[nb_layers].T)      # Forward hidden layer\n",
    "            b = np.asarray(self.bias_vectors[nb_layers])          # Forward bias\n",
    "            W = np.asarray(self.weights_vectors[nb_layers])       # Current layer weights\n",
    "            W_L = np.asarray(self.weights_vectors[nb_layers+1])   # Forward layer weights\n",
    " \n",
    "            dh = np.matmul(W_L.T,dh_L)* h * (1 - h)\n",
    "            if nb_layers == 0: \n",
    "                dW = (1./m) * np.matmul(dh, x_input) \n",
    "                \n",
    "            else:\n",
    "                dW = (1./m) * np.matmul(dh, self.hidden_vectors[nb_layers-1])\n",
    "                \n",
    "            db = (1./m)*np.sum(dh, axis=1, keepdims=True)\n",
    "\n",
    "            # compute momentum terms\n",
    "            V_dW = np.asarray(self.momentum_weights_vectors[nb_layers])\n",
    "            V_db = np.asarray(self.momentum_bias_vectors[nb_layers])\n",
    "            \n",
    "            self.momentum_weights_vectors[nb_layers] = (beta * V_dW + (1. - beta) * dW)\n",
    "            self.momentum_bias_vectors[nb_layers] = (beta * V_db  + (1. - beta) * db)\n",
    "                \n",
    "            # Update parameter using momentum\n",
    "            self.weights_vectors[nb_layers] = W - lr * V_dW\n",
    "            self.bias_vectors[nb_layers]    = b - lr * V_db\n",
    "            \n",
    "            dh_L = dh    \n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_test, Y_test):\n",
    "        \"\"\"Computes predicted output through feedforward method considering a test dataset.\"\"\"\n",
    "        \n",
    "        predictions = []\n",
    "        accurate = 0\n",
    "        n = X_test.shape[0]\n",
    "\n",
    "        # For all samples of X_test (no one-hot encode)\n",
    "        for x in range(n):\n",
    "            \n",
    "            _, y_pred = self.feedforward(X_test[x].reshape(1,X_test.shape[1]))\n",
    "            predictions.append(np.where(y_pred == np.amax(y_pred), 1., 0.).T[0])\n",
    "            \n",
    "        # Computes test accuracy\n",
    "        for y in range(len(predictions)):\n",
    "            \n",
    "            if np.array_equal(predictions[y], Y_test[y]):\n",
    "                \n",
    "                accurate += 1\n",
    "        accuracy = accurate / Y_test.shape[0]\n",
    "        \n",
    "        # calculer Accuracy, TPR, FPR, Precision, F1-score, recall ---> sickitlearn\n",
    "        return predictions, accuracy\n",
    "    \n",
    "    def trainingProcess(self, nb_epoch, batch_size, lr, beta):\n",
    "        \"\"\" Creates random mini-batch and apply training routine over each epoch \n",
    "        (feedforward following by backpropagation for each batch). Output training, validation \n",
    "        accuracy and loss function value over epochs. Prints loss function minimum and training \n",
    "        accuracy graph.\"\"\"\n",
    "        \n",
    "        m = self.input.shape[0]\n",
    "        train_loss = []\n",
    "        train_accuracies = []\n",
    "\n",
    "        batches = -(-m // batch_size)\n",
    "        print(\"nb of batches :\", batches)\n",
    "        \n",
    "        X_train, X_val, Y_train, Y_val = train_test_split(self.input, self.y, test_size=0.20)\n",
    "    \n",
    "        for e in range(nb_epoch):\n",
    "            \n",
    "            # Creates random minibatch\n",
    "            X_train_shuffled, Y_train_shuffled = shuffle(X_train, Y_train)\n",
    "            \n",
    "            average_cost = 0\n",
    "            for b in range(batches):\n",
    "                \n",
    "                begin = b * batch_size\n",
    "                end = min(begin + batch_size, X_train.shape[0]-1)\n",
    "                \n",
    "                X_batch = X_train_shuffled[begin:end]                \n",
    "                Y_batch = Y_train_shuffled[begin:end]\n",
    "   \n",
    "                if X_batch.shape[0] != 0: # Prevent empty batch\n",
    "                    \n",
    "                    forward, y_output = self.feedforward(np.asarray(X_batch))\n",
    "                    y_output = y_output.T\n",
    "\n",
    "                    train_cost = CrossEntropyLoss(y_output, Y_batch)\n",
    "                    average_cost += train_cost\n",
    "\n",
    "                    backprob = self.backpropagation(y_output, Y_batch, lr, beta, np.asarray(X_batch))\n",
    "\n",
    "                \n",
    "            # Validation accuracy:\n",
    "            pred_val, val_acc = self.predict(X_val, Y_val)\n",
    "            \n",
    "            # Train accuracy:\n",
    "            pred, train_acc = self.predict(self.input, self.y)\n",
    "            \n",
    "            print(\" * Epoch {}: Average cost = {}, Train_acc = {}, Val_acc = {}  * \".format(e+1, average_cost/batches, train_acc, val_acc))\n",
    "            print(\"*******************************************************\")\n",
    "            \n",
    "            # Store average training loss  and training accuracy value\n",
    "            epoch_loss = average_cost/batches\n",
    "            train_loss.append(epoch_loss)\n",
    "            train_accuracies.append(train_acc)\n",
    "            \n",
    "        # Print loss and accuracy graph\n",
    "        epoch = np.arange(0, nb_epoch, 1)\n",
    "        plt.plot(epoch, train_loss)\n",
    "        plt.savefig(\"loss.png\")\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss function minimum')\n",
    "        plt.title('Train loss function w.r.t epochs')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.plot(epoch, train_accuracies)\n",
    "        plt.savefig(\"acc.png\")\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Training Accuracy')\n",
    "        plt.title('Train accuracy w.r.t epochs')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading dataset\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X_data = iris.data\n",
    "Y_data = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Shuffles dataset\n",
    "X_data, Y_data = shuffle(X_data, Y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Noramlizes the data\n",
    "X_normalized = normalize(X_data,axis=0)\n",
    "X_data = X_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split dataset \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test and train one-hot encoding\n",
    "\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "Y_train_enc = label_encoder.fit_transform(Y_train)\n",
    "Y_test_enc = label_encoder.fit_transform(Y_test)\n",
    "\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "Y_train_enc = Y_train_enc.reshape(len(Y_train_enc), 1)\n",
    "Y_test_enc = Y_test_enc.reshape(len(Y_test_enc), 1)\n",
    "\n",
    "Y_train_enc = onehot_encoder.fit_transform(Y_train_enc)\n",
    "Y_test_enc = onehot_encoder.fit_transform(Y_test_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creates empty feedforward network\n",
    "model = MultiLayerPeceptron(X_train, Y_train_enc, nb_class=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adding hidden layers\n",
    "model = model.addHiddenLayer(64, dropout=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Running training routine\n",
    "model = model.trainingProcess(nb_epoch=500, batch_size=1, lr=.04, beta=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prediction on test dataset:\n",
    "pred, test_acc = model.predict(X_test, Y_test_enc)\n",
    "print(\"*** Test Accuracy ***: {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
